{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFNET\n",
    "## Desenvolvimento de Data-Driven Apps com Python [24E4_3] - TP2\n",
    "### Aluno: Rodrigo Moreira Avila\n",
    "---\n",
    "\n",
    "Repositório GIT: https://github.com/r-moreira/infnet-data-driven-apps-tp2-p2\n",
    "\n",
    "#### Questões teóricas\n",
    "\n",
    "> Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questão 4: Com base na implementação da Questão 2 (Parte 2), explique as principais limitações de utilizar LangChain para integrar a API da OpenAI.\n",
    "\n",
    "Discuta os seguintes aspectos:\n",
    "\n",
    "- Latência de resposta.\n",
    "- Limites de uso da API da OpenAI.\n",
    "- Desafios de escalabilidade e custo.\n",
    "- Qualidade das traduções geradas em comparação com outros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Latência de resposta**: O modelo da Open AI possui uma latência baixa, pois roda na infraestrutura da Open AI que já é otimizada para processamento de modelos de linguagem.\n",
    "\n",
    "2. **Limites de uso da API da OpenAI**:  A OpenAI possui limites de uso da API, que podem ser consultados em https://beta.openai.com/docs/api-reference/usage-limits. A API da OpenAI é cobrada por uso, e o custo pode ser um limitante para a utilização em larga escala.\n",
    "\n",
    "3. **Desafios de escalabilidade e custo**: Por ser uma API paga, a OpenAI possui uma infraestrutura robusta e escalável, o que facilita a utilização em larga escala, porém o custo pode ser um limitante para a utilização.\n",
    "\n",
    "4. **Qualidade das traduções geradas em comparação com outros modelos**: A qualidade das traduções geradas pelo modelo da OpenAI é muito boa, me parece ser um dos melhores disponíveis no mercado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Questão 5: Com base na aplicação desenvolvida na 3 (Parte 2), explique as limitações de usar LangChain para integrar o modelo HuggingFace de tradução.\n",
    "\n",
    "Discuta aspectos como:\n",
    "\n",
    "- Desempenho e tempo de resposta.\n",
    "- Consumo de recursos computacionais.\n",
    "- Possíveis limitações no ajuste fino do modelo.\n",
    "- Comparação com o uso direto da API HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A performance pode variar dependendo do tamanho do modelo e da infraestrutura. Modelos maiores podem exigir mais recursos computacionais. O ajuste fino do modelo pode ser limitado, pois o modelo é pré-treinado e não é possível ajustar os pesos do modelo. \n",
    "\n",
    "O uso direto da API HuggingFace pode ser mais eficiente, pois a API é otimizada para o modelo e possui uma infraestrutura robusta e escalável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questão 6: Com base nas questões 1-2 (Parte 1) e 2-3 (Parte 2), desenvolva uma tabela comparativa que aborde os seguintes critérios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Facilidade de uso/configuração.\n",
    "- Latência e desempenho.\n",
    "- Flexibilidade para diferentes modelos.\n",
    "- Custo e escalabilidade.\n",
    "- Adequação para protótipos versus aplicações em produção.\n",
    "- A comparação deve ser apresentada em formato de tabela, com colunas dedicadas a cada critério e linhas comparando - - - FastAPI puro com LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Critério                          | HuggingFace e Open AI (diretamente)        | LangChain                              |\n",
    "|-----------------------------------|--------------------------------------------|----------------------------------------|\n",
    "| **Facilidade de uso/configuração**| Requer configuração manual de modelos e pipelines. Pode ser mais complexo para iniciantes. | Abstrai grande parte da configuração, facilitando o uso inicial. |\n",
    "| **Latência e desempenho**         | Depende da implementação e otimização manual. Pode ser mais rápido se bem otimizado. | Pode introduzir alguma latência adicional devido à abstração, mas ainda eficiente. |\n",
    "| **Flexibilidade para diferentes modelos** | Alta flexibilidade, permite uso de qualquer modelo disponível no HuggingFace. | Flexível, mas pode ser limitado às integrações suportadas pela biblioteca. |\n",
    "| **Custo e escalabilidade**        | Custo depende do uso de infraestrutura própria ou serviços de terceiros. Escalabilidade manual. | Pode ser mais caro devido à abstração e serviços adicionais. Escalabilidade facilitada. |\n",
    "| **Adequação para protótipos versus aplicações em produção** | Bom para protótipos e produção, mas requer mais esforço para escalar e manter. | Excelente para protótipos rápidos e aplicações em produção com menos esforço de manutenção. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook rodrigo_avila_DR3_TP2_p2.ipynb to webpdf\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 227856 bytes to rodrigo_avila_DR3_TP2_p2.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to webpdf rodrigo_avila_DR3_TP2_p2.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
